{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "42033be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "48809283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askDistilBertOne(context, question):\n",
    "    # need to change this so that accomadates text larger than 512 words\n",
    "    model_path = 'model/distilbert-custom'\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    encoding = tokenizer([context], [question], padding='max_length', max_length=512)\n",
    "    \n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    train_loader = DataLoader(dataset, batch_size=1)\n",
    "    \n",
    "    model.to(torch.device('cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = batch['input_ids'].to(torch.device('cpu'))\n",
    "            attention_mask = batch['attention_mask'].to(torch.device('cpu'))\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "            answer_start_index = torch.argmax(outputs['start_logits'], dim=1)\n",
    "            answer_end_index = torch.argmax(outputs['end_logits'], dim=1)\n",
    "    \n",
    "    # some of the tokens include punctuation char so need to make sure there are no spaces around them\n",
    "    output = \"\"\n",
    "    for i in range(answer_start_index, answer_end_index + 1):\n",
    "        output += tokenizer.decode(encoding['input_ids'][0][i], skip_special_tokens=True).replace(\" \", \"\") + \" \"\n",
    "        output = output[:-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3ee3e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'posttrackedhundredsofposts'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"To better assess the impact of the current wave — which projections suggest could claim more than 1 million lives in 2023 — The Washington Post tracked hundreds of posts on popular Chinese platforms, including Weibo and Douyin, and reviewed material that was reposted on Twitter and other sites. The Post’s preliminary analysis found evidence of overwhelmed health-care facilities in major cities, particularly along the country’s heavily populated east coast.\"\n",
    "question = \"Where was the reviewed material reposted?\"\n",
    "output = askDistilBertOne(context, question)\n",
    "output # wrong answer need to figure out what changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "edeb302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askDistilBertTwo(context, question):\n",
    "    model_path = 'model/distilbert-custom'\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    inputs = tokenizer(context, question, return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    \n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "94db7bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter and other sites.'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"To better assess the impact of the current wave — which projections suggest could claim more than 1 million lives in 2023 — The Washington Post tracked hundreds of posts on popular Chinese platforms, including Weibo and Douyin, and reviewed material that was reposted on Twitter and other sites. The Post’s preliminary analysis found evidence of overwhelmed health-care facilities in major cities, particularly along the country’s heavily populated east coast.\"\n",
    "question = \"Where was the reviewed material reposted?\"\n",
    "output = askDistilBertTwo(context, question)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
